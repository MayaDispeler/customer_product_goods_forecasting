{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "storage_account_name = \"<your storage_account_name>\"\n",
    "container_name = \"<your 2nd container name>\"\n",
    "sas_token = \"<your SAS token>\"\n",
    "\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account_name}.dfs.core.windows.net\", \"SAS\")\n",
    "spark.conf.set(f\"fs.azure.sas.token.provider.type.{storage_account_name}.dfs.core.windows.net\", \n",
    "               \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.sas.fixed.token.{storage_account_name}.dfs.core.windows.net\", sas_token)\n",
    "\n",
    "parquet_path = \"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/curated_sales_data.parquet/\"\n",
    "df_curated = spark.read.parquet(parquet_path)\n",
    "\n",
    "df_curated.show(5)\n",
    "df_curated.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import dayofweek, weekofyear, month, year, to_date, col, when\n",
    "\n",
    "df_curated = df_curated.withColumn(\"sales_date\", to_date(col(\"sales_date\"), \"yyyy-MM-dd\"))\n",
    "df_fe = (df_curated\n",
    "    .withColumn(\"day_of_week\", dayofweek(col(\"sales_date\")))   # returns 1=Sunday, 2=Monday, ...\n",
    "    .withColumn(\"week_of_year\", weekofyear(col(\"sales_date\")))\n",
    "    .withColumn(\"month\", month(col(\"sales_date\")))\n",
    "    .withColumn(\"year\", year(col(\"sales_date\")))\n",
    ")\n",
    "\n",
    "df_fe.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df_fe = df_fe.withColumn(\"is_holiday\", when(col(\"month\") == 1, 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Partition by store_id, product_id, ordered by sales_date ascending\n",
    "windowSpec = Window.partitionBy(\"store_id\", \"product_id\").orderBy(\"sales_date\")\n",
    "\n",
    "# 7-Day Rolling Sales\n",
    "df_fe = df_fe.withColumn(\"rolling_7d_qty\",\n",
    "    F.avg(col(\"sales_qty\")).over(windowSpec.rowsBetween(-6, 0))\n",
    ")\n",
    "\n",
    "df_fe.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "promotions_df = spark.read.option(\"header\",\"true\") \\\n",
    "    .csv(f\"abfss://{your 1st container name}@{storage_account_name}.dfs.core.windows.net/promotions.csv\")\n",
    "\n",
    "# Convert date columns to DateType\n",
    "promotions_df = promotions_df \\\n",
    "    .withColumn(\"promo_start\", to_date(col(\"promo_start\"), \"yyyy-MM-dd\")) \\\n",
    "    .withColumn(\"promo_end\", to_date(col(\"promo_end\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# Join on store_id, product_id, and filter by date range\n",
    "joined_df = df_fe.join(\n",
    "    promotions_df,\n",
    "    on=[\"store_id\", \"product_id\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Then create a new column is_promo_active\n",
    "joined_df = joined_df.withColumn(\n",
    "    \"is_promo_active\",\n",
    "    when(\n",
    "        (col(\"sales_date\") >= col(\"promo_start\")) & (col(\"sales_date\") <= col(\"promo_end\")),\n",
    "        1\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "joined_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "output_path = f\"abfss://{your first container name}@{storage_account_name}.dfs.core.windows.net/cpg_features.parquet\"\n",
    "joined_df.write.mode(\"overwrite\").parquet(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df_check = spark.read.parquet(output_path)\n",
    "df_check.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(f\"abfss://raw@{storage_account_name}.dfs.core.windows.net/\"))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
