{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "storage_account_name = \"<your storage_account_name>\"\n",
    "sas_token = \"<your SAS token>\"\n",
    "\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account_name}.dfs.core.windows.net\", \"SAS\")\n",
    "spark.conf.set(f\"fs.azure.sas.token.provider.type.{storage_account_name}.dfs.core.windows.net\", \n",
    "               \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.sas.fixed.token.{storage_account_name}.dfs.core.windows.net\", sas_token)\n",
    "\n",
    "# 1. Copy from ADLS back to local driver:\n",
    "dbutils.fs.cp(\n",
    "    \"abfss://{your first container name}@{storage_account_name}.dfs.core.windows.net/cpg_forecast_rf.pkl\",\n",
    "    \"file:/tmp/cpg_forecast_rf.pkl\"\n",
    ")\n",
    "\n",
    "# 2. Load it with joblib\n",
    "model = joblib.load(\"/tmp/cpg_forecast_rf.pkl\")\n",
    "\n",
    "# 3. Confirm it's loaded\n",
    "print(\"Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "future_path = \"abfss://{your first container name}@{storage_account_name}.dfs.core.windows.net/future_data.csv\"\n",
    "df_future = spark.read.option(\"header\",\"true\").csv(future_path)\n",
    "df_future.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import dayofweek, weekofyear, month, year, to_date, col, when\n",
    "\n",
    "df_future = df_future.withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\")) \\\n",
    "                     .withColumn(\"day_of_week\", dayofweek(col(\"date\"))) \\\n",
    "                     .withColumn(\"week_of_year\", weekofyear(col(\"date\"))) \\\n",
    "                     .withColumn(\"month\", month(col(\"date\"))) \\\n",
    "                     .withColumn(\"year\", year(col(\"date\")))\n",
    "\n",
    "# If you have a holiday or promotion logic, replicate it here\n",
    "df_future = df_future.withColumn(\"is_holiday\", when(col(\"month\") == 1, 1).otherwise(0)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df_future_pd = df_future.toPandas()\n",
    "\n",
    "feature_cols = [\n",
    "   \"day_of_week\", \"week_of_year\", \"month\", \"year\",\n",
    "   \"is_holiday\", \"is_promo_active\", \n",
    "   \"price\"\n",
    "   # If you are skipping rolling_7d_qty for future predictions, remove it\n",
    "]\n",
    "\n",
    "X_future = df_future_pd[feature_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_future = model.predict(X_future)\n",
    "\n",
    "df_future_pd[\"predicted_sales_qty\"] = y_pred_future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df_preds_spark = spark.createDataFrame(df_future_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "output_path = f\"abfss://{your new container name for storing prediction}@{storage_account_name}.dfs.core.windows.net/future_predictions.parquet\"\n",
    "df_preds_spark.write.mode(\"overwrite\").parquet(output_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
