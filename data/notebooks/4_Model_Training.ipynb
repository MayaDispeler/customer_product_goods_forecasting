{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Start/attach to your cluster\n",
    "storage_account_name = \"< your storage_account_name>\"\n",
    "sas_token = \"<your SAS token>\"\n",
    "\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account_name}.dfs.core.windows.net\", \"SAS\")\n",
    "spark.conf.set(f\"fs.azure.sas.token.provider.type.{storage_account_name}.dfs.core.windows.net\", \n",
    "               \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.sas.fixed.token.{storage_account_name}.dfs.core.windows.net\", sas_token)\n",
    "\n",
    "feature_path = f\"abfss://<your first container name>@{storage_account_name}.dfs.core.windows.net/cpg_features.parquet\"\n",
    "df_features = spark.read.parquet(feature_path)\n",
    "df_features.show(5)\n",
    "df_features.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Example: filter rows before 2023-03-01 for training, after for test\n",
    "train_df = df_features.filter(col(\"sales_date\") < \"2023-03-01\")\n",
    "test_df = df_features.filter(col(\"sales_date\") >= \"2023-01-01\")\n",
    "\n",
    "print(\"Train rows:\", train_df.count())\n",
    "print(\"Test rows:\", test_df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train_pd = train_df.toPandas()\n",
    "test_pd = test_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "feature_cols = [\n",
    "    \"day_of_week\", \"week_of_year\", \"month\", \"year\",\n",
    "    \"is_holiday\", \"is_promo_active\", \n",
    "    \"rolling_7d_qty\"\n",
    "]\n",
    "target_col = \"sales_qty\"\n",
    "\n",
    "X_train = train_pd[feature_cols]\n",
    "y_train = train_pd[target_col]\n",
    "X_test = test_pd[feature_cols]\n",
    "y_test = test_pd[target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# USING SKLEARN\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Ensure X_test is not empty\n",
    "if X_test is None or X_test.shape[0] == 0:\n",
    "    raise ValueError(\"X_test is empty. Please provide a valid test dataset.\")\n",
    "\n",
    "# Create the regressor\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"MSE: {mse:.2f}, R2: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#USING TENSORFLOW\n",
    "%pip install --upgrade typing_extensions\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model_tf = tf.keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(len(feature_cols),)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1)  # single numeric output\n",
    "])\n",
    "\n",
    "model_tf.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Convert X_train, y_train to numpy arrays\n",
    "X_train_np = X_train.values\n",
    "y_train_np = y_train.values\n",
    "X_test_np = X_test.values\n",
    "y_test_np = y_test.values\n",
    "\n",
    "model_tf.fit(X_train_np, y_train_np, validation_split=0.1, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate\n",
    "loss = model_tf.evaluate(X_test_np, y_test_np)\n",
    "print(\"Test MSE:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "local_path = \"/tmp/cpg_forecast_rf.pkl\"\n",
    "joblib.dump(model, local_path)\n",
    "\n",
    "# Then copy from local driver to ADLS via dbutils or PySpark\n",
    "container_name = \"model-store\"\n",
    "storage_path = f\"abfss://{your first container name}@{storage_account_name}.dfs.core.windows.net/cpg_forecast_rf.pkl\"\n",
    "\n",
    "# Easiest approach might be a \"dbutils.fs.cp\" from file:/ to abfss:/\n",
    "dbutils.fs.cp(f\"file:{local_path}\", storage_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
